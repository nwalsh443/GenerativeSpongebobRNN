{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\valoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FinalBob.ipynb\n",
    "#Noah Walsh, Ben Valois, Jake Hamilton, Derek Windahl\n",
    "#Miniproject 2. In this project, we use an RNN trained on \n",
    "#Spongebob scripts to generate a Spongebob episode\n",
    "#NLTK might need to be installed via anaconda prompt \n",
    "#10/25/2018\n",
    "\n",
    "# imports needed\n",
    "import numpy\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "#nltk used for natural language processing\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our file and convert to a consistent case\n",
    "#make sure file is in the same directory as the notebook\n",
    "filename = \"Spng.txt\"\n",
    "raw_text = open(filename).read()\n",
    "#set text to lower case\n",
    "raw_text = raw_text.lower()\n",
    "#remove all punctuation\n",
    "for char in [\"[\",\"]\",\".\",\"?\",\"'\",\"#\",\"\\n\",\",\",\"!\",\"@\",\"$\",\"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"+\",\"-\",\"_\",\";\",\":\",\"<\",\">\",'\"',\"{\",\"}\",\"\\\\\",\"/\",\"~\",\"`\"]:\n",
    "    if char in raw_text:\n",
    "        raw_text = raw_text.replace(char,\"\")\n",
    "#tokenize words           \n",
    "proc_text = nltk.word_tokenize(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map our words to integers so that we can use them properly\n",
    "#chars = sorted(list(set(raw_text)))\n",
    "#char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "words = sorted(list(set(proc_text)))\n",
    "word_to_int = dict((w, i) for i, w in enumerate(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  10617\n"
     ]
    }
   ],
   "source": [
    "#split our text into our X and Y vectors\n",
    "#n_chars = len(raw_text)\n",
    "#n_vocab = len(chars)\n",
    "\n",
    "myWords = len(proc_text)\n",
    "myVocab = len(words)\n",
    "#number of words per sequence\n",
    "seq_length = 20\n",
    "dataX = []\n",
    "dataY = []\n",
    "#creates sequences/patterns\n",
    "for i in range(0, myWords - seq_length, 1):\n",
    "  seq_in = proc_text[i:i + seq_length]\n",
    "  seq_out = proc_text[i + seq_length]\n",
    "  dataX.append([word_to_int[word] for word in seq_in])\n",
    "  dataY.append(word_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10617"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#work with the resulting data to make sure that it's in a form that keras will take\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length))\n",
    "#X = X / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)\n",
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/57\n",
      "10617/10617 [==============================] - 30s 3ms/step - loss: 6.5278: 0s - loss: 6.53\n",
      "Epoch 2/57\n",
      "10617/10617 [==============================] - 8s 743us/step - loss: 6.2293 0s - los\n",
      "Epoch 3/57\n",
      "10617/10617 [==============================] - 8s 743us/step - loss: 6.1679\n",
      "Epoch 4/57\n",
      "10617/10617 [==============================] - 8s 746us/step - loss: 6.0060\n",
      "Epoch 5/57\n",
      "10617/10617 [==============================] - 8s 741us/step - loss: 5.9082\n",
      "Epoch 6/57\n",
      "10617/10617 [==============================] - 8s 743us/step - loss: 5.8199\n",
      "Epoch 7/57\n",
      "10617/10617 [==============================] - 8s 742us/step - loss: 5.7220\n",
      "Epoch 8/57\n",
      "10617/10617 [==============================] - 8s 741us/step - loss: 5.6321\n",
      "Epoch 9/57\n",
      "10617/10617 [==============================] - 8s 740us/step - loss: 5.5272\n",
      "Epoch 10/57\n",
      "10617/10617 [==============================] - 8s 741us/step - loss: 5.4173\n",
      "Epoch 11/57\n",
      "10617/10617 [==============================] - 8s 744us/step - loss: 5.2933\n",
      "Epoch 12/57\n",
      "10617/10617 [==============================] - 8s 742us/step - loss: 5.1690\n",
      "Epoch 13/57\n",
      "10617/10617 [==============================] - 8s 741us/step - loss: 5.0610\n",
      "Epoch 14/57\n",
      "10617/10617 [==============================] - 8s 744us/step - loss: 4.9412\n",
      "Epoch 15/57\n",
      "10617/10617 [==============================] - 8s 741us/step - loss: 4.8258\n",
      "Epoch 16/57\n",
      "10617/10617 [==============================] - 8s 743us/step - loss: 4.6931\n",
      "Epoch 17/57\n",
      "10617/10617 [==============================] - 8s 742us/step - loss: 4.5373\n",
      "Epoch 18/57\n",
      "10617/10617 [==============================] - 8s 747us/step - loss: 4.3991\n",
      "Epoch 19/57\n",
      "10617/10617 [==============================] - 8s 744us/step - loss: 4.2462\n",
      "Epoch 20/57\n",
      "10617/10617 [==============================] - 8s 747us/step - loss: 4.0942\n",
      "Epoch 21/57\n",
      "10617/10617 [==============================] - 8s 746us/step - loss: 3.9437\n",
      "Epoch 22/57\n",
      "10617/10617 [==============================] - 8s 743us/step - loss: 3.7625\n",
      "Epoch 23/57\n",
      "10617/10617 [==============================] - 8s 743us/step - loss: 3.5958\n",
      "Epoch 24/57\n",
      "10617/10617 [==============================] - 8s 745us/step - loss: 3.4319 0s - loss: 3.4\n",
      "Epoch 25/57\n",
      "10617/10617 [==============================] - 8s 746us/step - loss: 3.2664\n",
      "Epoch 26/57\n",
      "10617/10617 [==============================] - 8s 742us/step - loss: 3.0780\n",
      "Epoch 27/57\n",
      "10617/10617 [==============================] - 8s 744us/step - loss: 2.9076 0s - loss: 2.908\n",
      "Epoch 28/57\n",
      "10617/10617 [==============================] - 8s 748us/step - loss: 2.7375\n",
      "Epoch 29/57\n",
      "10617/10617 [==============================] - 8s 747us/step - loss: 2.5765\n",
      "Epoch 30/57\n",
      "10617/10617 [==============================] - 8s 746us/step - loss: 2.4261\n",
      "Epoch 31/57\n",
      "10617/10617 [==============================] - 8s 746us/step - loss: 2.2583 1s - \n",
      "Epoch 32/57\n",
      "10617/10617 [==============================] - 8s 744us/step - loss: 2.1374\n",
      "Epoch 33/57\n",
      "10617/10617 [==============================] - 8s 745us/step - loss: 2.0084\n",
      "Epoch 34/57\n",
      "10617/10617 [==============================] - 8s 744us/step - loss: 1.8710\n",
      "Epoch 35/57\n",
      "10617/10617 [==============================] - 8s 745us/step - loss: 1.7384\n",
      "Epoch 36/57\n",
      "10617/10617 [==============================] - 8s 748us/step - loss: 1.6073\n",
      "Epoch 37/57\n",
      "10617/10617 [==============================] - 8s 745us/step - loss: 1.4899 0s - los\n",
      "Epoch 38/57\n",
      "10617/10617 [==============================] - 8s 750us/step - loss: 1.4065\n",
      "Epoch 39/57\n",
      "10617/10617 [==============================] - 8s 743us/step - loss: 1.2902\n",
      "Epoch 40/57\n",
      "10617/10617 [==============================] - 8s 747us/step - loss: 1.2173\n",
      "Epoch 41/57\n",
      "10617/10617 [==============================] - 8s 747us/step - loss: 1.1216\n",
      "Epoch 42/57\n",
      "10617/10617 [==============================] - 8s 749us/step - loss: 1.0415\n",
      "Epoch 43/57\n",
      "10617/10617 [==============================] - 8s 747us/step - loss: 1.0246\n",
      "Epoch 44/57\n",
      "10617/10617 [==============================] - 8s 743us/step - loss: 1.0220\n",
      "Epoch 45/57\n",
      "10617/10617 [==============================] - 8s 746us/step - loss: 0.9004\n",
      "Epoch 46/57\n",
      "10617/10617 [==============================] - 8s 745us/step - loss: 0.8157\n",
      "Epoch 47/57\n",
      "10617/10617 [==============================] - 8s 749us/step - loss: 0.7384\n",
      "Epoch 48/57\n",
      "10617/10617 [==============================] - 8s 790us/step - loss: 0.6877\n",
      "Epoch 49/57\n",
      "10617/10617 [==============================] - 8s 771us/step - loss: 0.6321\n",
      "Epoch 50/57\n",
      "10617/10617 [==============================] - 8s 755us/step - loss: 0.5934\n",
      "Epoch 51/57\n",
      "10617/10617 [==============================] - 8s 751us/step - loss: 0.5405\n",
      "Epoch 52/57\n",
      "10617/10617 [==============================] - 8s 747us/step - loss: 0.5070\n",
      "Epoch 53/57\n",
      "10617/10617 [==============================] - 8s 749us/step - loss: 0.4730\n",
      "Epoch 54/57\n",
      "10617/10617 [==============================] - 8s 748us/step - loss: 0.4213\n",
      "Epoch 55/57\n",
      "10617/10617 [==============================] - 8s 747us/step - loss: 0.4036\n",
      "Epoch 56/57\n",
      "10617/10617 [==============================] - 8s 749us/step - loss: 0.3771\n",
      "Epoch 57/57\n",
      "10617/10617 [==============================] - 8s 747us/step - loss: 0.3547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2801cd6ecf8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the model\n",
    "#RNN hidden dimension\n",
    "hidden_size = 800\n",
    "num_steps = seq_length\n",
    "\n",
    "model = Sequential()\n",
    "#perform word embedding \n",
    "model.add(Embedding(myVocab, hidden_size, input_length = num_steps))\n",
    "#long short term memory layer (LSTM layer)\n",
    "model.add(LSTM(hidden_size, input_shape=(X.shape), return_sequences=True))\n",
    "#dropout to improve generated text\n",
    "model.add(Dropout(0.5))\n",
    "#2nd LSTM layer\n",
    "model.add(LSTM(hidden_size))\n",
    "model.add(Dropout(0.5))\n",
    "#final normalized layer\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#model fit with 57 epochs found to be ideal number\n",
    "model.fit(X, y, epochs=57, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" by the power invested in me i now pronounce you man and wife manager enters the cafeteria what is going  \"\n",
      "\n",
      "Done.\n",
      " on in here mermaid man points to spongebob who stops smiling you may kiss the bride spongebob gets kicked out and rolls home patrick did you reunite our heroes spongebob no but im married back at the retirement home mermaid man he and barnacle boy are in rocking chairs up up and away up up and away spongebob dressed up as a woman and speaking in a southern accent oh mah this purse is so big and heavy patrick dressed up as a robber hold it right there maam ill be taking that grabs spongebobs purse spongebob screams haylp haylp haylp patrick its working spongebob barnacle boy comes over to him wha are you here to rescue little ol me barnacle boy yells at spongebob ruining his makeup pipe down you could wake mermaid man and hes ornery when his nap is disturbed spongebob approaches mermaid man who is sleeping with his eyes open ever alert mermaid man has trained himself to sleep with his eyes open barnacle boy confound it get away from him mermaid man wakes up stop shoutin im nappin barnacle boy close up to his face its not me you ol coot retired elderly 1 yes retired elderly 2 thats me retired elderly 3 im over here spongebob excuse me mermaid man mermaid man what do you want barnacle boy this better be good spongebob thisll cheer you up were almost done painting your invisible boatmobile the invisible boatmobile is shown almost completely covered in black paint and we also see patrick smiling while holding a paintbrush with black paint on it barnacle boy gah its supposed to be invisible thats it we got ta end our life of leisure its time to come out of retirement theres evil afoot mermaid man evil where is it barnacle boy there it is points at spongebob and patrick you know what this means opens up a box with shiny rings mermaid man donuts barnacle boy oh brother puts the ring on mermaid man and then they try to put them together but they miss so they do it again but it doesnt work say the oath mermaid man in his young voice mermaid man and barnacle boy unite they put the rings together and this time it works throw a waterball at em spongebob and patrick waterballs waterballs barnacle boy throws a waterball at spongebob but nothing happens patrick hehehehehe barnacle boy mumbling mories its not working mermaid man mermaid man heheshes absorbing it like some kind of evil sponge mermaid man and barnacle boy dogpaddle away they swim around spongebob and patrick patrick its the raging whirlpool spongebob and patrick cheer as the whirlpool consumes them mermaid man those fiends theyre actually enjoying it spongebob and patrick do it again do it again do it again barnacle boy now what mermaid man we need help mermaid man and barnacle boy sea creatures unite they summon the sea creatures but they are retired so they come slowly barnacle\n"
     ]
    }
   ],
   "source": [
    "#create our prediction\n",
    "#get word from integer\n",
    "int_to_word = dict((i, w) for i, w in enumerate(words))\n",
    "oot = \"\"\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([(int_to_word[value]+\" \") for value in pattern]), \"\\\"\")\n",
    "# generates 500 words\n",
    "for i in range(500):\n",
    "  x = numpy.reshape(pattern, (1, len(pattern)))\n",
    "  #x = x / float(n_vocab)\n",
    "  prediction = model.predict(x, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_word[index]\n",
    "  seq_in = [int_to_word[value] for value in pattern]\n",
    "  oot = oot +\" \"+ result\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n",
    "print(oot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
